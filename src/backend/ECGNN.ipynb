{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e996070",
   "metadata": {},
   "source": [
    "## 1. ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1eaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dotenv huggingface_hub datasets sentence_transformers langchain_google_genai chromadb neo4j langchain_community py2neo spacy --upgrade torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cpu\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "TORCH = print(torch.__version__)          \n",
    "CUDA = print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9906d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "SEED = 42\n",
    "load_dotenv()\n",
    "login(token=os.getenv('HF_TOKEN'))\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables.\")\n",
    "\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc335f",
   "metadata": {},
   "source": [
    "## 2. Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade50b1f",
   "metadata": {},
   "source": [
    "data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c2373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Qasper dataset\n",
    "from datasets import load_dataset, Dataset\n",
    "dataset = load_dataset(\"allenai/qasper\", cache_dir='./data/Qasper/qasper_cache')\n",
    "\n",
    "# shuffle\n",
    "trainset = dataset['train'] #.shuffle(seed=SEED)\n",
    "validset = dataset['validation'] #.shuffle(seed=SEED)\n",
    "testset  = dataset['test'] #.shuffle(seed=SEED)\n",
    "# subset = trainset.select(range(5,10))  # select a subset of 1000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2650630",
   "metadata": {},
   "source": [
    "data preprocessing: ensure data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da04f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List\n",
    "\n",
    "FLOAT_TAG = \"FLOAT SELECTED: \"          # **대소문자 구분**\n",
    "REF_TAG = re.compile(r\" BIBREF\\d+\")\n",
    "# FIG_TAG = re.compile(r\" FIGREF\\d+\")\n",
    "\n",
    "def _clear_tag(ev_list: List[str]) -> List[str]:\n",
    "    \"\"\"evidence 리스트에서 'FLOAT SELECTED: ' 부분 문자열을 삭제하고\n",
    "       내용이 남은 evidence만 반환\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for ev in ev_list:\n",
    "        new_ev = ev.replace(FLOAT_TAG, \"\")\n",
    "        new_ev = REF_TAG.sub(\"\", new_ev)\n",
    "        new_ev = new_ev.strip()\n",
    "        if new_ev:                       # 치환 후 내용이 남아 있을 때만 보존\n",
    "            cleaned.append(new_ev)\n",
    "    return cleaned\n",
    "\n",
    "def filter_sample(sample: Dict) -> Dict:\n",
    "    \"\"\"Qasper 샘플에서 'FLOAT SELECTED: ' 태그를 제거하고\n",
    "       내용이 남지 않는 answer-detail / question 을 드롭\n",
    "    \"\"\"\n",
    "    new_qas = {k: [] for k in sample[\"qas\"]}\n",
    "\n",
    "    for idx in range(len(sample[\"qas\"][\"question\"])):\n",
    "        blk = deepcopy(sample[\"qas\"][\"answers\"][idx])\n",
    "\n",
    "        kept_det, kept_ann, kept_wid = [], [], []\n",
    "        for det, ann, wid in zip(blk[\"answer\"],\n",
    "                                 blk.get(\"annotation_id\", []),\n",
    "                                 blk.get(\"worker_id\", [])):\n",
    "            cleaned_ev = _clear_tag(det.get(\"evidence\", []))\n",
    "            if cleaned_ev:                               # evidence가 하나라도 남을 때만 keep\n",
    "                det = deepcopy(det)\n",
    "                det[\"evidence\"] = cleaned_ev\n",
    "                kept_det.append(det)\n",
    "                kept_ann.append(ann)\n",
    "                kept_wid.append(wid)\n",
    "\n",
    "        if kept_det:                                    # 질문 유지 여부\n",
    "            blk.update(\n",
    "                answer        = kept_det,\n",
    "                annotation_id = kept_ann,\n",
    "                worker_id     = kept_wid,\n",
    "            )\n",
    "            new_qas[\"answers\"].append(blk)\n",
    "            for k in sample[\"qas\"]:\n",
    "                if k != \"answers\":\n",
    "                    new_qas[k].append(sample[\"qas\"][k][idx])\n",
    "\n",
    "    # Update the sample full_text paragraph with cleaned QAs\n",
    "    for j, paragraph in enumerate(sample['full_text']['paragraphs']):\n",
    "        for sub_paragraph in paragraph:\n",
    "            # Clean the sub-paragraph text\n",
    "            cleaned_text = sub_paragraph.replace(FLOAT_TAG, \"\")\n",
    "            cleaned_text = REF_TAG.sub(\"\", cleaned_text).strip()\n",
    "            # cleaned_text = FIG_TAG.sub(\"\", cleaned_text).strip()\n",
    "\n",
    "            sub_paragraph = cleaned_text\n",
    "\n",
    "    sample[\"qas\"] = new_qas\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf53f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filtering function to every sample in the trainset.\n",
    "trainset = trainset.map(filter_sample)\n",
    "validset = validset.map(filter_sample)\n",
    "testset = testset.map(filter_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92615969",
   "metadata": {},
   "source": [
    "validate data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ef90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_evidence_clean(sample: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    ① evidence가 비어 있지 않은지\n",
    "    ② evidence 안에 placeholder 문자열이 남아 있지 않은지\n",
    "    모두 만족하면 True\n",
    "    \"\"\"\n",
    "    ok = True\n",
    "    for q_idx, blk in enumerate(sample[\"qas\"][\"answers\"]):\n",
    "        for a_idx, det in enumerate(blk[\"answer\"]):\n",
    "            ev = det.get(\"evidence\", [])\n",
    "            if not ev:\n",
    "                print(f\"[오류] Q{q_idx}-A{a_idx}: evidence가 비어 있습니다.\")\n",
    "                ok = False\n",
    "                continue\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b515a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_no_float_selected(sample: Dict) -> bool:\n",
    "    \"\"\"모든 evidence에 'FLOAT SELECTED: ' 문자열이 남아 있지 않은지 확인\"\"\"\n",
    "    ok = True\n",
    "    for q_idx, blk in enumerate(sample[\"qas\"][\"answers\"]):\n",
    "        for a_idx, det in enumerate(blk[\"answer\"]):\n",
    "            for ev in det.get(\"evidence\", []):\n",
    "                if FLOAT_TAG in ev:\n",
    "                    print(f\"[오류] Q{q_idx}-A{a_idx}: 미제거 태그 발견 → “{ev}”\")\n",
    "                    ok = False\n",
    "    return ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "347a2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_not_clean = True\n",
    "def validate_all_samples(dataset):\n",
    "    \"\"\"모든 샘플에 대해 evidence가 비어 있거나 미제거 태그가 있는지 확인\"\"\"\n",
    "    for i, sample in enumerate(dataset):\n",
    "        if not validate_evidence_clean(sample):\n",
    "            print(f\"[오류] 샘플 #{i}에 빈 evidence가 존재합니다.\")\n",
    "            return False\n",
    "        if not validate_no_float_selected(sample):\n",
    "            print(f\"[오류] 샘플 #{i}에 미제거 태그가 존재합니다.\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "if not validate_all_samples(trainset):\n",
    "    raise ValueError(\"Trainset validation failed.\")\n",
    "if not validate_all_samples(validset):\n",
    "    raise ValueError(\"Validset validation failed.\")\n",
    "if not validate_all_samples(testset):\n",
    "    raise ValueError(\"Testset validation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73c6c4",
   "metadata": {},
   "source": [
    "***\n",
    "## New Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94c75693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, hashlib, itertools, shutil, json, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "\n",
    "HF_CACHE_FOLDER     = \"./data/.cache/scibert-nli\"\n",
    "# CHROMA_DIR          = \"./demo_chroma_db\"\n",
    "CHROMA_DIR          = \"./chroma_qasper_10\"\n",
    "COLLECTION_SENT_NAME     = \"paper_sentences\"\n",
    "COLLECTION_RELA_NAME     = \"paper_relations\"\n",
    "\n",
    "\n",
    "# Clean previous run\n",
    "# if Path(CHROMA_DIR).exists():\n",
    "#     shutil.rmtree(CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f7fc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name gsarti/scibert-nli. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▸ Loading SentenceTransformer …\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"▸ Loading SentenceTransformer …\")\n",
    "# os.environ['HF_HOME'] = './data/.cache'  # Set cache directory for HuggingFace models\n",
    "ENC_MODEL = SentenceTransformer('gsarti/scibert-nli', cache_folder=HF_CACHE_FOLDER, device=DEVICE)\n",
    "if not ENC_MODEL:\n",
    "    raise RuntimeError(\"Failed to load the SentenceTransformer model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089fd03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▸ Gemini 1.5 Flash loaded (LangChain wrapper).\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "LLM = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.0\n",
    ")\n",
    "if not LLM:\n",
    "    raise RuntimeError(\"Failed to load the Gemini 1.5 Flash model.\")\n",
    "else: print(\"▸ Gemini 1.5 Flash loaded (LangChain wrapper).\")\n",
    "\n",
    "LABELS = [\"Claim\", \"Evidence\", \"Background\", \"Method\", \"Result\",\n",
    "          \"Interpretation\", \"Contrast\", \"Cause-Effect\",\n",
    "          \"Temporal\", \"Condition\", \"Other\"]\n",
    "LABEL2IDX = {lab: i for i, lab in enumerate(LABELS)}\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "[MUST FOLLOW Task]:\n",
    "1. You are an academic discourse analyst.\n",
    "2. Classify how Sentence_B is related to Sentence_A using EXACTLY ONE \"Allowed labels\".\n",
    "3. If none apply, output \"Other\".\n",
    "\n",
    "[Allowed labels]:\n",
    "Claim, Evidence, Background, Method, Result, Interpretation,\n",
    "Contrast, Cause-Effect, Temporal, Condition, Other\n",
    "\n",
    "[Sentence pairs]:\n",
    "{PAIRS}\n",
    "\n",
    "Respond only with a list of labels, in order:\n",
    "1: <label>\n",
    "2: <label>\n",
    "...\n",
    "{N}: <label>\n",
    "\"\"\"\n",
    "\n",
    "def one_hot_encode_labels(labels: list[str]) -> list[list[float]]:\n",
    "    index = {l: i for i, l in enumerate(LABELS)}\n",
    "    vecs = []\n",
    "    for l in labels:\n",
    "        vec = [0.0] * len(LABELS)\n",
    "        vec[index.get(l, -1)] = 1.0 if l in index else 0.0\n",
    "        vecs.append(vec)\n",
    "    return vecs\n",
    "\n",
    "def format_sentence_pairs(pairs: list[tuple[str, str]]) -> str:\n",
    "    return \"\\n\".join([\n",
    "        f\"{i+1}.\\nSentence_A: {a}\\nSentence_B: {b}\"\n",
    "        for i, (a, b) in enumerate(pairs)\n",
    "    ])\n",
    "\n",
    "def parse_llm_labels(response: str, expected: int) -> List[str]:\n",
    "    lines = response.strip().splitlines()\n",
    "    labels = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if \":\" not in line:\n",
    "            continue                   # 형식 일치하지 않으면 스킵\n",
    "        _, raw = line.split(\":\", 1)\n",
    "        lbl = raw.strip()\n",
    "        if not lbl or lbl not in LABELS:\n",
    "            lbl = \"Other\"  # 유효하지 않은 라벨은 \"Other\"로 대체\n",
    "        labels.append(lbl)\n",
    "\n",
    "    # 부족한 개수만큼 패딩\n",
    "    if len(labels) < expected:\n",
    "        labels.extend([\"Other\"] * (expected - len(labels)))\n",
    "\n",
    "    # 초과하면 앞 expected개만 사용\n",
    "    return labels[:expected]\n",
    "\n",
    "import re\n",
    "SPLIT_PUNCS = re.compile(r\"[.!?;:]+\")\n",
    "\n",
    "def split_sentences(paragraph: str) -> list[str]:\n",
    "    \"\"\"문장 구분 기호로 문단을 분할\"\"\"\n",
    "    sentences = SPLIT_PUNCS.split(paragraph.strip())\n",
    "    return [s.strip() for s in sentences if s.strip()]  # 빈 문자열 제거\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8e7cb",
   "metadata": {},
   "source": [
    " Initialising ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e53fef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▸ Initialising ChromaDB …\n",
      "./demo_chroma_db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(182, 126)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "print(\"▸ Initialising ChromaDB …\")\n",
    "CHROMA_DIR = './demo_chroma_db'\n",
    "print(CHROMA_DIR)\n",
    "client = chromadb.PersistentClient(path=CHROMA_DIR)\n",
    "col_sent  = client.get_or_create_collection(name=COLLECTION_SENT_NAME)\n",
    "col_rel   = client.get_or_create_collection(name=COLLECTION_RELA_NAME)\n",
    "col_sent.count(),   col_rel.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ba10522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, hashlib\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict\n",
    "from typing import Dict, Iterable, Tuple, Optional\n",
    "\n",
    "def generate_sent_nodes(dataset: DatasetDict) -> Iterable[Dict]:\n",
    "    for sample in tqdm(dataset, total=len(dataset), desc=\"Generating nodes\"):\n",
    "        paper_id = sample[\"id\"]\n",
    "    \n",
    "        for sec_idx, (sec_name, sec_content) in enumerate(\n",
    "                zip(sample[\"full_text\"][\"section_name\"],\n",
    "                    sample[\"full_text\"][\"paragraphs\"])):\n",
    "    \n",
    "            for para_idx, para in enumerate(sec_content):\n",
    "                sentences = split_sentences(para.strip())\n",
    "                prev: Optional[Tuple[str, str]] = None\n",
    "\n",
    "                for sent_idx, sent in enumerate(filter(None, sentences)):\n",
    "                    para_id = f\"{paper_id}/sec{sec_idx}/para{para_idx}\"\n",
    "                    path = f\"{para_id}/sent{sent_idx}\"\n",
    "                    sid  = hashlib.sha1(path.encode()).hexdigest()\n",
    "\n",
    "                    yield {\n",
    "                        \"sid\":    sid,\n",
    "                        \"sent\":   sent.strip(),\n",
    "                        \"prev\":   prev,\n",
    "                        \"meta\":   dict(paper_id=paper_id, sec_idx=sec_idx,\n",
    "                                        para_idx=para_idx, sent_idx=sent_idx)\n",
    "                    }\n",
    "                    prev = dict(sid=sid, sent=sent.strip())  # skips first sentence in paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dae9bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "from typing import List, Tuple\n",
    "\n",
    "def flush_sentence(ids, sents, metas):\n",
    "    if not ids:\n",
    "        return\n",
    "\n",
    "    embs = ENC_MODEL.encode(\n",
    "        sents,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        batch_size=len(sents),\n",
    "        show_progress_bar=False,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    col_sent.upsert(\n",
    "        ids=ids,\n",
    "        embeddings=embs,\n",
    "        documents=sents,\n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "async def flush_pairs_async(\n",
    "    pairs: List[Tuple[str, str, str, str, str]],  # (paper_id, sid_src, sid_dst, sent_prev, sent_curr)\n",
    "    batch_size: int = 50,\n",
    ") -> None:\n",
    "    if not pairs:\n",
    "        logging.debug(\"flush_pairs_async: Received empty pair buffer. Skipping.\")\n",
    "        return\n",
    "\n",
    "    ab_pairs = [(sent_prev, sent_curr) for (_, _, _, sent_prev, sent_curr) in pairs]\n",
    "    logging.info(f\"↪ [flush_pairs_async] Processing {len(pairs)}\")\n",
    "    \n",
    "    prompt_txt = format_sentence_pairs(ab_pairs)\n",
    "    prompt     = PROMPT.format(PAIRS=prompt_txt, N=len(pairs))\n",
    "\n",
    "    # 비동기 LLM 호출\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        resp = await LLM.ainvoke(prompt)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[LLM Error] flush_pairs_async failed: {e}\")\n",
    "        return\n",
    "    t1 = time.time()\n",
    "    logging.info(f\"LLM response received in {t1 - t0:.2f}s\")\n",
    "\n",
    "    labels = parse_llm_labels(resp.content, expected=len(pairs))\n",
    "    onehot = one_hot_encode_labels(labels)\n",
    "\n",
    "    # Chroma upsert\n",
    "    t2 = time.time()\n",
    "    success, failed = 0, 0\n",
    "\n",
    "    for (paper_id, sid_src, sid_dst, _, _), lab, vec in zip(pairs, labels, onehot):\n",
    "        rel_id = f\"{sid_src}|{sid_dst}\"\n",
    "\n",
    "        try:\n",
    "            col_rel.upsert(\n",
    "                ids        = [rel_id],\n",
    "                documents  = [f\"{sid_src} <REL> {sid_dst}\"],\n",
    "                embeddings = [vec],\n",
    "                metadatas  = [{\n",
    "                    \"relation_label\": lab,\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"sid_src\": sid_src,\n",
    "                    \"sid_dst\": sid_dst\n",
    "                }],\n",
    "            )\n",
    "            logging.debug(f\"[Upsert Success] ID: {rel_id} → {lab}, paper_id: {paper_id}\")\n",
    "            success += 1\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[Upsert Failed] ID: {rel_id} — {e}\")\n",
    "            failed += 1\n",
    "\n",
    "    t3 = time.time()\n",
    "    logging.info(f\"✓ Upsert complete: {success} succeeded, {failed} failed (⏱ {t3 - t2:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed8d9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def process_dataset(nodes, batch_sent=32, batch_pairs=50):\n",
    "    ids, sents, metas = [], [], []\n",
    "    pair_buf          = []\n",
    "    tasks             = []\n",
    "\n",
    "    for node in nodes:\n",
    "        ids.append(node[\"sid\"])\n",
    "        sents.append(node[\"sent\"])\n",
    "        metas.append(node[\"meta\"])\n",
    "\n",
    "        if node[\"prev\"]:\n",
    "            # print(f\"Processing pair: {node['prev']['sid']} -> {node['sid']}\")\n",
    "            pair_buf.append((node['meta']['paper_id'],\n",
    "                             node[\"prev\"][\"sid\"],  node[\"sid\"],\n",
    "                             node[\"prev\"][\"sent\"], node[\"sent\"]))\n",
    "\n",
    "        if len(ids) >= batch_sent:\n",
    "            flush_sentence(ids, sents, metas)\n",
    "            ids, sents, metas = [], [], []\n",
    "\n",
    "        if len(pair_buf) >= batch_pairs:\n",
    "            # 병렬 실행 예약\n",
    "            tasks.append(asyncio.create_task(flush_pairs_async(pair_buf)))\n",
    "            pair_buf = []\n",
    "\n",
    "    if ids:\n",
    "        flush_sentence(ids, sents, metas)\n",
    "    if pair_buf:\n",
    "        tasks.append(asyncio.create_task(flush_pairs_async(pair_buf)))\n",
    "\n",
    "    # 모든 병렬 작업 종료 대기\n",
    "    if tasks:\n",
    "        await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9171c8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▸ Embedding sentences + relations …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating nodes: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-16 16:27:52,587 [INFO] ↪ [flush_pairs_async] Processing 50\n",
      "2025-06-16 16:27:52,593 [INFO] ↪ [flush_pairs_async] Processing 50\n",
      "2025-06-16 16:27:52,594 [INFO] ↪ [flush_pairs_async] Processing 26\n",
      "2025-06-16 16:27:54,639 [INFO] LLM response received in 2.04s\n",
      "2025-06-16 16:27:55,087 [INFO] ✓ Upsert complete: 26 succeeded, 0 failed (⏱ 0.45s)\n",
      "2025-06-16 16:27:55,100 [INFO] LLM response received in 2.51s\n",
      "2025-06-16 16:27:55,981 [INFO] ✓ Upsert complete: 50 succeeded, 0 failed (⏱ 0.88s)\n",
      "2025-06-16 16:27:55,983 [INFO] LLM response received in 3.40s\n",
      "2025-06-16 16:27:56,906 [INFO] ✓ Upsert complete: 50 succeeded, 0 failed (⏱ 0.92s)\n",
      "▸ Done embedding sentences + relations.\n",
      "▸ Stored sentences: 182\n",
      "▸ Stored vectors   : 126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"▸ Embedding sentences + relations …\")\n",
    "subset = trainset.select(range(1))  # for testing\n",
    "await process_dataset(generate_sent_nodes(subset))\n",
    "print(\"▸ Done embedding sentences + relations.\")\n",
    "print(f\"▸ Stored sentences: {col_sent.count():,}\")\n",
    "print(f\"▸ Stored vectors   : {col_rel.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264aa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "[Doc = 1811.00942]  'What aspects have been compared between various language models?'\n",
      " • doc = In truth, language models exist in a quality–performance tradeoff space\n",
      " • doc = In the present work, we describe and examine the tradeoff space between quality and performance for the task of language modeling\n",
      " • doc = In this paper, we examine the quality–performance tradeoff in the shift from non-neural to neural language models\n",
      " • doc = Quality–performance tradeoff\n",
      " • doc = Specifically focused on language modeling, this paper examines an issue that to our knowledge has not been explored\n"
     ]
    }
   ],
   "source": [
    "def semantic_query(query: str, paper_id: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    paper_id 에 해당하는 문서 내부에서만 최근접 문장 k개 검색\n",
    "    \"\"\"\n",
    "    q_vec = ENC_MODEL.encode(query, normalize_embeddings=True)\n",
    "    res   = col_sent.query(\n",
    "        query_embeddings=[q_vec],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "        # --- 핵심 ---\n",
    "        where={\"paper_id\": paper_id}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[Doc = {paper_id}]  {query!r}\")\n",
    "    for doc, dist, sid, meta in zip(res[\"documents\"][0], res[\"distances\"][0], res[\"ids\"][0], res[\"metadatas\"][0]):\n",
    "        print(f\" • doc = {doc}\")# dist={dist:.4f}  sid={sid[:8]} text≈{meta.get('sent_idx')}  rel={meta.get('relation_label')}\")\n",
    "\n",
    "\n",
    "sample = trainset[4]\n",
    "print(len(sample['qas']['question']))\n",
    "query = sample['qas']['question'][0]\n",
    "id = sample['id']\n",
    "topk= 5\n",
    "semantic_query(query, id, k=topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd766f4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eafc6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chroma_qasper_10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(126, 182)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the collections \n",
    "from chromadb import PersistentClient\n",
    "print(CHROMA_DIR)\n",
    "client = PersistentClient(path='./demo_chroma_db')\n",
    "col_sent =client.get_or_create_collection(name=COLLECTION_SENT_NAME)\n",
    "col_rel = client.get_or_create_collection(name=COLLECTION_RELA_NAME)\n",
    "col_rel.count(), col_sent.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f3903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['d4a889228628703d6f4d577c9c9ae6c44c2c5597|11c8af6284e5a0fc753e2900950071f30c31eb64'], 'embeddings': array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'documents': ['d4a889228628703d6f4d577c9c9ae6c44c2c5597 <REL> 11c8af6284e5a0fc753e2900950071f30c31eb64'], 'uris': None, 'included': ['documents', 'metadatas', 'embeddings'], 'data': None, 'metadatas': [{'paper_id': '1909.00694', 'sid_src': 'd4a889228628703d6f4d577c9c9ae6c44c2c5597', 'relation_label': 'Evidence', 'sid_dst': '11c8af6284e5a0fc753e2900950071f30c31eb64'}]}\n",
      "{'ids': ['d4a889228628703d6f4d577c9c9ae6c44c2c5597'], 'embeddings': None, 'documents': ['$\\\\lambda _{\\\\rm CA}$ was about one-third of $\\\\lambda _{\\\\rm CO}$, and this indicated that the CA pairs were noisier than the CO pairs'], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': [{'sent_idx': 1, 'sec_idx': 14, 'para_idx': 6, 'paper_id': '1909.00694'}]}\n"
     ]
    }
   ],
   "source": [
    "# col_rel 에서 확인용 아무 relation \n",
    "r = col_rel.get(\n",
    "    where={\"relation_label\": \"Evidence\"},\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"],\n",
    "    limit=1\n",
    ")\n",
    "print(r)\n",
    "# col_sent 에서 확인\n",
    "result = col_sent.get(ids=\"d4a889228628703d6f4d577c9c9ae6c44c2c5597\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347b2bb",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4819cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "u:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01   Avg Loss: 1.3877\n",
      "Epoch 02   Avg Loss: 1.3821\n",
      "Epoch 03   Avg Loss: 1.3782\n",
      "Epoch 04   Avg Loss: 1.3732\n",
      "Epoch 05   Avg Loss: 1.3678\n",
      "Epoch 06   Avg Loss: 1.3627\n",
      "Epoch 07   Avg Loss: 1.3529\n",
      "Epoch 08   Avg Loss: 1.3447\n",
      "Epoch 09   Avg Loss: 1.3409\n",
      "Epoch 10   Avg Loss: 1.3329\n",
      "Epoch 11   Avg Loss: 1.3275\n",
      "Epoch 12   Avg Loss: 1.3204\n",
      "Epoch 13   Avg Loss: 1.3021\n",
      "Epoch 14   Avg Loss: 1.3009\n",
      "Epoch 15   Avg Loss: 1.2991\n",
      "Epoch 16   Avg Loss: 1.2912\n",
      "Epoch 17   Avg Loss: 1.2849\n",
      "Epoch 18   Avg Loss: 1.2722\n",
      "Epoch 19   Avg Loss: 1.2668\n",
      "Epoch 20   Avg Loss: 1.2566\n",
      "Epoch 21   Avg Loss: 1.2603\n",
      "Epoch 22   Avg Loss: 1.2629\n",
      "Epoch 23   Avg Loss: 1.2690\n",
      "Epoch 24   Avg Loss: 1.2431\n",
      "Epoch 25   Avg Loss: 1.2290\n",
      "Epoch 26   Avg Loss: 1.2332\n",
      "Epoch 27   Avg Loss: 1.2276\n",
      "Epoch 28   Avg Loss: 1.2202\n",
      "Epoch 29   Avg Loss: 1.2417\n",
      "Epoch 30   Avg Loss: 1.1849\n",
      "Learned relation weights: tensor([1.0000, 1.0234, 1.0312, 1.0248, 1.0197, 1.0083, 1.0001, 1.0317, 1.0000,\n",
      "        1.0282, 1.0299, 1.0264])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1) ChromaDB 에서 문장·관계 로드\n",
    "\n",
    "# 1.1) 문장 노드 피처 및 메타\n",
    "sent_data   = col_sent.get(include=[\"embeddings\",\"metadatas\"])\n",
    "node_ids    = sent_data[\"ids\"]                                 # List[str]\n",
    "feats       = torch.tensor(sent_data[\"embeddings\"], dtype=torch.float)  # [N,768]\n",
    "sent_metas  = sent_data[\"metadatas\"]                           # 각 dict에 \"paper_id\",\"sid\",\"para_id\",\"sent_idx\" 포함\n",
    "\n",
    "# 1.2) 관계 엣지 데이터\n",
    "rel_data   = col_rel.get(include=[\"metadatas\"])\n",
    "rel_metas  = rel_data[\"metadatas\"]\n",
    "label_set  = [\"Claim\",\"Evidence\",\"Background\",\"Method\",\"Result\",\n",
    "              \"Interpretation\",\"Contrast\",\"Cause-Effect\",\n",
    "              \"Temporal\",\"Condition\",\"Other\"]\n",
    "lab2idx    = {l:i for i,l in enumerate(label_set)}\n",
    "NUM_REL    = len(label_set) + 1   # +1 for paragraph edge type\n",
    "PARA_TYPE  = len(label_set)       # index for paragraph-level edges\n",
    "\n",
    "# sid → global index\n",
    "id2idx = {sid:i for i,sid in enumerate(node_ids)}\n",
    "\n",
    "# 2) 문서별 서브그래프 + paragraph 엣지 구성\n",
    "docs = defaultdict(lambda: {\"nodes\": set(), \"rel_edges\": [], \"rel_types\": []})\n",
    "\n",
    "# 2.1) 문장 기여 관계(rel) 추가\n",
    "for m in rel_metas:\n",
    "    pid = m[\"paper_id\"]\n",
    "    u   = id2idx[m[\"sid_src\"]]\n",
    "    v   = id2idx[m[\"sid_dst\"]]\n",
    "    t   = lab2idx[m[\"relation_label\"]]\n",
    "    docs[pid][\"nodes\"].update({u,v})\n",
    "    docs[pid][\"rel_edges\"].append((u,v))\n",
    "    docs[pid][\"rel_types\"].append(t)\n",
    "\n",
    "# 2.2) paragraph-level 이웃 추가 (sent_idx 차이 1)\n",
    "#    para_id별로 노드 인덱스와 sent_idx를 그룹화\n",
    "para2nodes = defaultdict(list)\n",
    "for idx, meta in enumerate(sent_metas):\n",
    "    para2nodes[(meta[\"paper_id\"], meta[\"para_idx\"])].append((meta[\"sent_idx\"], idx))\n",
    "\n",
    "for (pid, _), seq in para2nodes.items():\n",
    "    # 같은 문서·문단 내에서 sent_idx 순으로 정렬\n",
    "    seq_sorted = sorted(seq, key=lambda x: x[0])\n",
    "    # 인접 페어만 이웃으로 연결\n",
    "    for (_, u), (_, v) in zip(seq_sorted, seq_sorted[1:]):\n",
    "        docs[pid][\"nodes\"].update({u,v})\n",
    "        docs[pid][\"rel_edges\"].append((u,v))\n",
    "        docs[pid][\"rel_types\"].append(PARA_TYPE)\n",
    "        # undirected 처리하려면 역방향도 추가\n",
    "        docs[pid][\"rel_edges\"].append((v,u))\n",
    "        docs[pid][\"rel_types\"].append(PARA_TYPE)\n",
    "\n",
    "# 2.3) Data 리스트 생성\n",
    "data_list = []\n",
    "for pid, info in docs.items():\n",
    "    if not info[\"rel_edges\"]:\n",
    "        continue\n",
    "    uniq_nodes = sorted(info[\"nodes\"])\n",
    "    g2l        = {g:i for i,g in enumerate(uniq_nodes)}\n",
    "    x_sub      = feats[uniq_nodes]  # [n_sub,768]\n",
    "\n",
    "    # 엣지 및 타입\n",
    "    e_list = info[\"rel_edges\"]\n",
    "    t_list = info[\"rel_types\"]\n",
    "    edge_index = torch.tensor([\n",
    "        [g2l[u] for u,_ in e_list],\n",
    "        [g2l[v] for _,v in e_list]\n",
    "    ], dtype=torch.long)\n",
    "    edge_type  = torch.tensor(t_list, dtype=torch.long)\n",
    "\n",
    "    data_list.append(Data(x=x_sub, edge_index=edge_index, edge_type=edge_type))\n",
    "\n",
    "# 3) Relation-Weighted GraphSAGE 정의\n",
    "class RelGraphSAGE(MessagePassing):\n",
    "    def __init__(self, in_dim, hid_dim, num_rel):\n",
    "        super().__init__(aggr=\"mean\")\n",
    "        self.lin_self  = torch.nn.Linear(in_dim, hid_dim)\n",
    "        self.lin_neigh = torch.nn.Linear(in_dim, hid_dim)\n",
    "        self.w_rel     = torch.nn.Parameter(torch.ones(num_rel), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        h_self = self.lin_self(x)\n",
    "        h_neigh = self.propagate(edge_index, x=x, edge_type=edge_type)\n",
    "        return F.relu(h_self + h_neigh)\n",
    "\n",
    "    def message(self, x_j, edge_type):\n",
    "        w = self.w_rel[edge_type].unsqueeze(-1)    # [E,1]\n",
    "        return w * self.lin_neigh(x_j)\n",
    "\n",
    "# 4) 학습 세팅\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model     = RelGraphSAGE(in_dim=768, hid_dim=256, num_rel=NUM_REL).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs    = 30\n",
    "loader    = DataLoader(data_list, batch_size=1, shuffle=True)\n",
    "\n",
    "# 5) Contrastive-style 학습\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 노드 임베딩 계산\n",
    "        h = model(batch.x, batch.edge_index, batch.edge_type)  # [n_sub,256]\n",
    "\n",
    "        # positive edge similarity\n",
    "        src, dst  = batch.edge_index\n",
    "        pos_sim   = (h[src] * h[dst]).sum(dim=1)               # [E]\n",
    "        w_pos     = model.w_rel[batch.edge_type]               # [E]\n",
    "\n",
    "        # negative sampling\n",
    "        neg_idx   = negative_sampling(batch.edge_index,\n",
    "                                      num_nodes=h.size(0),\n",
    "                                      num_neg_samples=src.size(0))\n",
    "        ns, nd    = neg_idx\n",
    "        neg_sim   = (h[ns] * h[nd]).sum(dim=1)\n",
    "\n",
    "        # loss 계산\n",
    "        loss_pos  = - (w_pos * F.logsigmoid(pos_sim)).sum() / w_pos.sum()\n",
    "        loss_neg  = - F.logsigmoid(-neg_sim).mean()\n",
    "        loss      = loss_pos + loss_neg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {ep:02d}   Avg Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "# 6) 결과: 최종 노드 임베딩 & 관계 가중치\n",
    "model.eval()\n",
    "embeddings_by_doc = []\n",
    "with torch.no_grad():\n",
    "    for data in data_list:\n",
    "        data = data.to(device)\n",
    "        h_sub = model(data.x, data.edge_index, data.edge_type)\n",
    "        embeddings_by_doc.append(h_sub.cpu())\n",
    "learned_w = model.w_rel.detach().cpu()  # [num_rel]\n",
    "\n",
    "print(\"Learned relation weights:\", learned_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6afbb7",
   "metadata": {},
   "source": [
    "## 4. LangGraph State and Node config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "326760d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sent = client.get_or_create_collection(COLLECTION_SENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3) 벡터 인덱싱 → 문장 단위 DB 로딩으로 변경\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# %% LangGraph & 노드 정의 (이하는 기존과 동일)\n",
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "from typing import TypedDict\n",
    "\n",
    "# 모델 설정\n",
    "encoder     = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "ce_reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "llm         = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=os.getenv('GOOGLE_API_KEY'),\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# State Schema\n",
    "class InputState(TypedDict):\n",
    "    paper_id: str\n",
    "    question: str\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    paper_id: str\n",
    "    answer: str\n",
    "    top_docs: list[str]\n",
    "    top_metadatas: list[dict]\n",
    "\n",
    "class OverallState(InputState, OutputState):\n",
    "    retrieved_docs: list[str]\n",
    "    retrieved_metadatas: list[dict]\n",
    "\n",
    "# retrieve 노드: 문장 컬렉션에서 검색\n",
    "def retrieve(state: OverallState) -> OverallState:\n",
    "    assert col_sent is not None, \"Sentence-level collection not found\"\n",
    "    q        = state[\"question\"]\n",
    "    paper_id = state[\"paper_id\"]\n",
    "    # print(f\"Retrieving sentences for paper {paper_id!r} with query {q!r}\")\n",
    "    # 1) 질의 임베딩\n",
    "    q_emb = encoder.encode(q)\n",
    "    # 2) col_sent에서 논문별 필터링 후 top-20 문장 검색\n",
    "    res = col_sent.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=10,\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "        where={\"paper_id\": paper_id}\n",
    "    )\n",
    "    # print(f\"Retrived sents : {res['documents'][0]}\")\n",
    "    # print(f\"Retrieved {len(res['documents'][0])} sentences for paper {paper_id!r} with query {q!r}\")\n",
    "    # Chromadb 문법상 [0]으로 추출\n",
    "    state[\"retrieved_docs\"]      = res[\"documents\"][0]\n",
    "    state[\"retrieved_metadatas\"] = res[\"metadatas\"][0]\n",
    "    return state\n",
    "\n",
    "# rerank / generate 노드는 기존과 동일\n",
    "def rerank(state: OverallState) -> OverallState:\n",
    "    docs      = state[\"retrieved_docs\"]\n",
    "    metadatas = state[\"retrieved_metadatas\"]\n",
    "    q         = state[\"question\"]\n",
    "\n",
    "    scores = ce_reranker.predict([(q, d) for d in docs])\n",
    "    ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    state[\"top_docs\"]      = [docs[i] for i, _ in ranked[:6]]\n",
    "    state[\"top_metadatas\"] = [metadatas[i] for i, _ in ranked[:6]]\n",
    "    return state\n",
    "\n",
    "def generate(state: OverallState) -> OverallState:\n",
    "    ctx = \"\\n\\n\".join(state[\"top_docs\"])\n",
    "    prompt = (\n",
    "        f\"Prompt: Answer based on context below. If you don't know, say 'unanswerable'.\\n\"\n",
    "        f\"Context:\\n{ctx}\\n\\nQuestion: {state['question']}\\nAnswer:\"\n",
    "    )\n",
    "    resp = llm.invoke(prompt)\n",
    "    state[\"answer\"] = resp.content\n",
    "    return state\n",
    "\n",
    "# 파이프라인 빌드\n",
    "builder = StateGraph(state_schema=OverallState, input=InputState, output=OutputState)\n",
    "builder.add_node(\"retrieve\", retrieve)\n",
    "builder.add_node(\"rerank\",   rerank)\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.set_entry_point(\"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"rerank\")\n",
    "builder.add_edge(\"rerank\",   \"generate\")\n",
    "builder.set_finish_point(\"generate\")\n",
    "\n",
    "rag_pipeline = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0ada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/88 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     69\u001b[39m answers   = sample[\u001b[33m\"\u001b[39m\u001b[33mqas\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33manswers\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q_text, ans_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(questions, answers):\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# 3.1) RAG-invoke\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     out = \u001b[43mrag_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpaper_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_text\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     pred_ans        = out[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m].strip()\n\u001b[32m     75\u001b[39m     pred_metadatas  = out[\u001b[33m\"\u001b[39m\u001b[33mtop_metadatas\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mu:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mu:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mu:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:161\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    159\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mu:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mu:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mu:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mrerank\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     62\u001b[39m metadatas = state[\u001b[33m\"\u001b[39m\u001b[33mretrieved_metadatas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     63\u001b[39m q         = state[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m scores = \u001b[43mce_reranker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m ranked = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(scores), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     68\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mtop_docs\u001b[39m\u001b[33m\"\u001b[39m]      = [docs[i] \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m ranked[:\u001b[32m6\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mu:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mu:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\util.py:68\u001b[39m, in \u001b[36mcross_encoder_predict_rank_args_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m         kwargs.pop(deprecated_arg)\n\u001b[32m     64\u001b[39m         logger.warning(\n\u001b[32m     65\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe CrossEncoder.predict `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` argument is deprecated and has no effect. It will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mu:\\Workspace\\pythonWorkspace\\LangchainTest\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:628\u001b[39m, in \u001b[36mCrossEncoder.predict\u001b[39m\u001b[34m(self, sentences, batch_size, show_progress_bar, activation_fn, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    592\u001b[39m \u001b[33;03mPerforms predictions with the CrossEncoder on the given sentence pairs.\u001b[39;00m\n\u001b[32m    593\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    625\u001b[39m \u001b[33;03m        # => array([0.6912767, 0.4303499], dtype=float32)\u001b[39;00m\n\u001b[32m    626\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    627\u001b[39m input_was_singular = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mstr\u001b[39m):  \u001b[38;5;66;03m# Cast an individual pair to a list with length 1\u001b[39;00m\n\u001b[32m    629\u001b[39m     sentences = [sentences]\n\u001b[32m    630\u001b[39m     input_was_singular = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range",
      "During task with name 'rerank' and id '0d984b47-2a5b-d9ee-a467-e341ae605814'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# 1) 데이터 로드 (검증용 subset)\n",
    "dataset = load_dataset('allenai/qasper', split='train', cache_dir='./data/Qasper/qasper_cache')\n",
    "# subset  = dataset.shuffle(seed=42).select(range(int(len(dataset)*0.1)))  # 10%\n",
    "\n",
    "# 2) helper functions\n",
    "def normalize(text: str) -> str:\n",
    "    return ''.join(c.lower() for c in text if c.isalnum() or c.isspace()).strip()\n",
    "\n",
    "def find_gold_para_ids(sample, gold_evids):\n",
    "    paper_id = sample[\"id\"]\n",
    "    gold_ids = set()\n",
    "    # full_text['paragraphs'] 는 섹션별로 [문단1, 문단2, …] 리스트를 가집니다.\n",
    "    sections = sample[\"full_text\"][\"paragraphs\"]\n",
    "    for ev in gold_evids:\n",
    "        ev_norm = ev.strip()\n",
    "        for sec_idx, paras in enumerate(sections):\n",
    "            for para_idx, para_text in enumerate(paras):\n",
    "                if ev_norm in para_text:\n",
    "                    para_id = f\"sec{sec_idx}/para{para_idx}\"\n",
    "                    gold_ids.add(f\"{paper_id}_{para_id}\")\n",
    "                    # 하나의 evidence는 하나의 문단만 매핑하므로 찾으면 바로 빠져나감\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "    return gold_ids\n",
    "\n",
    "def compute_retrieval_metrics(pred: list[str], gold: set[str]):\n",
    "    pred_set = set(pred)\n",
    "    tp = len(pred_set & gold)\n",
    "    prec = tp / len(pred_set) if pred_set else 0.0\n",
    "    rec  = tp / len(gold)     if gold     else 0.0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "def compute_answer_metrics(pred_ans: str, gold_texts: list[str], gold_yesno: list[bool], gold_unans: list[bool]):\n",
    "    norm_pred = normalize(pred_ans)\n",
    "    # Exact Match over all annotators\n",
    "    em = any(norm_pred == normalize(gt) for gt in gold_texts)\n",
    "    # yes/no\n",
    "    if any(gold_yesno):\n",
    "        yn_pred = norm_pred.startswith('yes') or norm_pred.startswith('no')\n",
    "        yn_acc  = int(yn_pred and norm_pred.split()[0] == ('yes' if any(gold_yesno) else 'no'))\n",
    "    else:\n",
    "        yn_acc = None\n",
    "    # unanswerable\n",
    "    if any(gold_unans):\n",
    "        un_pred = norm_pred.startswith('unanswerable')\n",
    "        un_acc  = int(un_pred)\n",
    "    else:\n",
    "        un_acc = None\n",
    "    return int(em), yn_acc, un_acc\n",
    "\n",
    "# 3) evaluation loop\n",
    "records = []\n",
    "for sample in tqdm(subset, desc=\"Evaluating\"):\n",
    "    pid       = sample[\"id\"]\n",
    "    questions = sample[\"qas\"][\"question\"]\n",
    "    answers   = sample[\"qas\"][\"answers\"]\n",
    "    \n",
    "    for q_text, ans_block in zip(questions, answers):\n",
    "        # 3.1) RAG-invoke\n",
    "        out = rag_pipeline.invoke({\"paper_id\": pid, \"question\": q_text})\n",
    "        pred_ans        = out[\"answer\"].strip()\n",
    "        pred_metadatas  = out[\"top_metadatas\"]\n",
    "        # predicted evidence IDs\n",
    "        pred_ids = []\n",
    "        for md in pred_metadatas:\n",
    "            pid     = md[\"paper_id\"]\n",
    "            sec_idx = md[\"sec_idx\"]\n",
    "            para_idx= md[\"para_idx\"]\n",
    "            para_id = f\"sec{sec_idx}/para{para_idx}\"\n",
    "            pred_ids.append(f\"{pid}_{para_id}\")\n",
    "        # in format of \"paper_id_para_idx\"\n",
    "        \n",
    "        # 3.2) gold aggregation\n",
    "        gold_texts, gold_evids, gold_yesno, gold_unans = [], [], [], []\n",
    "        # del gold_texts, gold_yesno, gold_unans  # clear previous\n",
    "        for ann in ans_block[\"answer\"]:\n",
    "            if ann.get(\"answer\"):\n",
    "                gold_texts.append(ann[\"answer\"])\n",
    "            if ann.get(\"evidence\"):\n",
    "                gold_evids.extend(ann[\"evidence\"])\n",
    "            if ann.get(\"yes_no\") is not None:\n",
    "                gold_yesno.append(ann[\"yes_no\"])\n",
    "            if ann.get(\"unanswerable\") is not None:\n",
    "                gold_unans.append(ann[\"unanswerable\"])\n",
    "        \n",
    "        gold_ids = find_gold_para_ids(sample, gold_evids)\n",
    "        \n",
    "        # 3.3) metrics\n",
    "        prec, rec, f1 = compute_retrieval_metrics(pred_ids, gold_ids)\n",
    "        em, yn_acc, un_acc = compute_answer_metrics(pred_ans, gold_texts, gold_yesno, gold_unans)\n",
    "        \n",
    "        records.append({\n",
    "            \"paper_id\":           pid,\n",
    "            \"question\":           q_text,\n",
    "            \"precision\":          prec,\n",
    "            \"recall\":             rec,\n",
    "            \"f1\":                 f1,\n",
    "            \"exact_match\":        em,\n",
    "            \"yes_no_acc\":         yn_acc,\n",
    "            \"unans_acc\":          un_acc\n",
    "        })\n",
    "\n",
    "# 4) DataFrame 생성 및 paper별 집계\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# 질문 단위 전체 평균\n",
    "overall = df.mean(numeric_only=True).round(4)\n",
    "print(\"=== Overall ===\")\n",
    "print(overall.to_dict())\n",
    "\n",
    "# paper_id 별 평균\n",
    "by_paper = (\n",
    "    df\n",
    "    .groupby(\"paper_id\")\n",
    "    .agg({\n",
    "        \"precision\":   \"mean\",\n",
    "        \"recall\":      \"mean\",\n",
    "        \"f1\":          \"mean\",\n",
    "        \"exact_match\": \"mean\",\n",
    "        \"yes_no_acc\":  \"mean\",\n",
    "        \"unans_acc\":   \"mean\"\n",
    "    })\n",
    "    .round(4)\n",
    "    .reset_index()\n",
    ")\n",
    "print(\"\\n=== By Paper ===\")\n",
    "print(by_paper)\n",
    "\n",
    "# 5) 결과 저장\n",
    "df.to_csv(\"qasper_rag_eval_per_question.csv\", index=False)\n",
    "by_paper.to_csv(\"qasper_rag_eval_by_paper.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
