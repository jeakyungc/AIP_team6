{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e996070",
   "metadata": {},
   "source": [
    "## 1. ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab10d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dependencies...\")\n",
    "%pip install -U huggingface_hub chromadb langchain_google_genai sentence_transformers torch_geometric numpy tqdm datasets protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "TORCH = print(torch.__version__)          \n",
    "CUDA = print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9906d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "SEED = 42\n",
    "load_dotenv()\n",
    "login(token=os.getenv('HF_TOKEN'))\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables.\")\n",
    "\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc335f",
   "metadata": {},
   "source": [
    "## 2. Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade50b1f",
   "metadata": {},
   "source": [
    "data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Qasper dataset\n",
    "from datasets import load_dataset, Dataset\n",
    "dataset = load_dataset(\"allenai/qasper\", cache_dir='./data/Qasper/qasper_cache')\n",
    "\n",
    "# shuffle\n",
    "trainset = dataset['train'] #.shuffle(seed=SEED)\n",
    "# validset = dataset['validation'] #.shuffle(seed=SEED)\n",
    "# testset  = dataset['test'] #.shuffle(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2650630",
   "metadata": {},
   "source": [
    "data preprocessing: ensure data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List\n",
    "\n",
    "FLOAT_TAG = \"FLOAT SELECTED: \"  # evidence에서 제거할 태그\n",
    "REF_TAG = re.compile(r\" BIBREF\\d+\")\n",
    "# FIG_TAG = re.compile(r\" FIGREF\\d+\")\n",
    "\n",
    "def _clear_tag(ev_list: List[str]) -> List[str]:\n",
    "    \"\"\"evidence 리스트에서 'FLOAT SELECTED: ' 부분 문자열을 삭제하고\n",
    "       내용이 남은 evidence만 반환\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for ev in ev_list:\n",
    "        new_ev = ev.replace(FLOAT_TAG, \"\")\n",
    "        new_ev = REF_TAG.sub(\"\", new_ev)\n",
    "        new_ev = new_ev.strip()\n",
    "        if new_ev:                       # 치환 후 내용이 남아 있을 때만 보존\n",
    "            cleaned.append(new_ev)\n",
    "    return cleaned\n",
    "\n",
    "def filter_sample(sample: Dict) -> Dict:\n",
    "    \"\"\"Qasper 샘플에서 'FLOAT SELECTED: ' 태그를 제거하고\n",
    "       내용이 남지 않는 answer-detail / question 을 드롭\n",
    "    \"\"\"\n",
    "    new_qas = {k: [] for k in sample[\"qas\"]}\n",
    "\n",
    "    for idx in range(len(sample[\"qas\"][\"question\"])):\n",
    "        blk = deepcopy(sample[\"qas\"][\"answers\"][idx])\n",
    "\n",
    "        kept_det, kept_ann, kept_wid = [], [], []\n",
    "        for det, ann, wid in zip(blk[\"answer\"],\n",
    "                                 blk.get(\"annotation_id\", []),\n",
    "                                 blk.get(\"worker_id\", [])):\n",
    "            cleaned_ev = _clear_tag(det.get(\"evidence\", []))\n",
    "            if cleaned_ev:                               # evidence가 하나라도 남을 때만 keep\n",
    "                det = deepcopy(det)\n",
    "                det[\"evidence\"] = cleaned_ev\n",
    "                kept_det.append(det)\n",
    "                kept_ann.append(ann)\n",
    "                kept_wid.append(wid)\n",
    "\n",
    "        if kept_det:                                    # 질문 유지 여부\n",
    "            blk.update(\n",
    "                answer        = kept_det,\n",
    "                annotation_id = kept_ann,\n",
    "                worker_id     = kept_wid,\n",
    "            )\n",
    "            new_qas[\"answers\"].append(blk)\n",
    "            for k in sample[\"qas\"]:\n",
    "                if k != \"answers\":\n",
    "                    new_qas[k].append(sample[\"qas\"][k][idx])\n",
    "\n",
    "    # Update the sample full_text paragraph with cleaned QAs\n",
    "    for j, paragraph in enumerate(sample['full_text']['paragraphs']):\n",
    "        for sub_paragraph in paragraph:\n",
    "            # Clean the sub-paragraph text\n",
    "            cleaned_text = sub_paragraph.replace(FLOAT_TAG, \"\")\n",
    "            cleaned_text = REF_TAG.sub(\"\", cleaned_text).strip()\n",
    "            # cleaned_text = FIG_TAG.sub(\"\", cleaned_text).strip()\n",
    "\n",
    "            sub_paragraph = cleaned_text\n",
    "\n",
    "    sample[\"qas\"] = new_qas\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf53f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filtering function to every sample in the trainset.\n",
    "trainset = trainset.map(filter_sample)\n",
    "# validset = validset.map(filter_sample)\n",
    "# testset = testset.map(filter_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92615969",
   "metadata": {},
   "source": [
    "validate data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_evidence_clean(sample: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    1. evidence가 비어 있지 않은지\n",
    "    2. evidence 안에 placeholder 문자열이 남아 있지 않은지\n",
    "    모두 만족하면 True\n",
    "    \"\"\"\n",
    "    ok = True\n",
    "    for q_idx, blk in enumerate(sample[\"qas\"][\"answers\"]):\n",
    "        for a_idx, det in enumerate(blk[\"answer\"]):\n",
    "            ev = det.get(\"evidence\", [])\n",
    "            if not ev:\n",
    "                print(f\"[오류] Q{q_idx}-A{a_idx}: evidence가 비어 있습니다.\")\n",
    "                ok = False\n",
    "                continue\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b515a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_no_float_selected(sample: Dict) -> bool:\n",
    "    \"\"\"모든 evidence에 'FLOAT SELECTED: ' 문자열이 남아 있지 않은지 확인\"\"\"\n",
    "    ok = True\n",
    "    for q_idx, blk in enumerate(sample[\"qas\"][\"answers\"]):\n",
    "        for a_idx, det in enumerate(blk[\"answer\"]):\n",
    "            for ev in det.get(\"evidence\", []):\n",
    "                if FLOAT_TAG in ev:\n",
    "                    print(f\"[오류] Q{q_idx}-A{a_idx}: 미제거 태그 발견 → “{ev}”\")\n",
    "                    ok = False\n",
    "    return ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_not_clean = True\n",
    "def validate_all_samples(dataset):\n",
    "    \"\"\"모든 샘플에 대해 evidence가 비어 있거나 미제거 태그가 있는지 확인\"\"\"\n",
    "    for i, sample in enumerate(dataset):\n",
    "        if not validate_evidence_clean(sample):\n",
    "            print(f\"[오류] 샘플 #{i}에 빈 evidence가 존재합니다.\")\n",
    "            return False\n",
    "        if not validate_no_float_selected(sample):\n",
    "            print(f\"[오류] 샘플 #{i}에 미제거 태그가 존재합니다.\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "if not validate_all_samples(trainset):\n",
    "    raise ValueError(\"Trainset validation failed.\")\n",
    "# if not validate_all_samples(validset):\n",
    "#     raise ValueError(\"Validset validation failed.\")\n",
    "# if not validate_all_samples(testset):\n",
    "#     raise ValueError(\"Testset validation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73c6c4",
   "metadata": {},
   "source": [
    "***\n",
    "## New Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c75693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, hashlib, itertools, shutil, json, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "\n",
    "HF_CACHE_FOLDER     = \"./data/.cache/scibert-nli\"\n",
    "CHROMA_DIR          = \"./demo_chroma_db\"\n",
    "# CHROMA_DIR          = \"./chroma_qasper_10\"\n",
    "COLLECTION_SENT_NAME     = \"paper_sentences\"\n",
    "COLLECTION_RELA_NAME     = \"paper_relations\"\n",
    "\n",
    "\n",
    "# Clean previous run\n",
    "# if Path(CHROMA_DIR).exists():\n",
    "#     shutil.rmtree(CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"▸ Loading SentenceTransformer …\")\n",
    "# os.environ['HF_HOME'] = './data/.cache'  # Set cache directory for HuggingFace models\n",
    "ENC_MODEL = SentenceTransformer('gsarti/scibert-nli', cache_folder=HF_CACHE_FOLDER, device=DEVICE)\n",
    "if not ENC_MODEL:\n",
    "    raise RuntimeError(\"Failed to load the SentenceTransformer model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089fd03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "print(\"▸ Loading Gemini 1.5 Flash model …\")\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "LLM = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.0\n",
    ")\n",
    "if not LLM:\n",
    "    raise RuntimeError(\"Failed to load the Gemini 1.5 Flash model.\")\n",
    "else: print(\"▸ Gemini 1.5 Flash loaded (LangChain wrapper).\")\n",
    "\n",
    "LABELS = [\"Claim\", \"Evidence\", \"Background\", \"Method\", \"Result\",\n",
    "          \"Interpretation\", \"Contrast\", \"Cause-Effect\",\n",
    "          \"Temporal\", \"Condition\", \"Other\"]\n",
    "LABEL2IDX = {lab: i for i, lab in enumerate(LABELS)}\n",
    "\n",
    "# Not used in this script, but use for further study\n",
    "PROMPT = \"\"\"\\ \n",
    "[MUST FOLLOW Task]:\n",
    "1. You are an academic discourse analyst.\n",
    "2. Classify how Sentence_B is related to Sentence_A using EXACTLY ONE \"Allowed labels\".\n",
    "3. If none apply, output \"Other\".\n",
    "\n",
    "[Allowed labels]:\n",
    "Claim, Evidence, Background, Method, Result, Interpretation,\n",
    "Contrast, Cause-Effect, Temporal, Condition, Other\n",
    "\n",
    "[Sentence pairs]:\n",
    "{PAIRS}\n",
    "\n",
    "Respond only with a list of labels, in order:\n",
    "1: <label>\n",
    "2: <label>\n",
    "...\n",
    "{N}: <label>\n",
    "\"\"\"\n",
    "\n",
    "def one_hot_encode_labels(labels: list[str]) -> list[list[float]]:\n",
    "    index = {l: i for i, l in enumerate(LABELS)}\n",
    "    vecs = []\n",
    "    for l in labels:\n",
    "        vec = [0.0] * len(LABELS)\n",
    "        vec[index.get(l, -1)] = 1.0 if l in index else 0.0\n",
    "        vecs.append(vec)\n",
    "    return vecs\n",
    "\n",
    "def format_sentence_pairs(pairs: list[tuple[str, str]]) -> str:\n",
    "    return \"\\n\".join([\n",
    "        f\"{i+1}.\\nSentence_A: {a}\\nSentence_B: {b}\"\n",
    "        for i, (a, b) in enumerate(pairs)\n",
    "    ])\n",
    "\n",
    "def parse_llm_labels(response: str, expected: int) -> List[str]:\n",
    "    lines = response.strip().splitlines()\n",
    "    labels = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if \":\" not in line:\n",
    "            continue                   # 형식 일치하지 않으면 스킵\n",
    "        _, raw = line.split(\":\", 1)\n",
    "        lbl = raw.strip()\n",
    "        if not lbl or lbl not in LABELS:\n",
    "            lbl = \"Other\"  # 유효하지 않은 라벨은 \"Other\"로 대체\n",
    "        labels.append(lbl)\n",
    "\n",
    "    # 부족한 개수만큼 패딩\n",
    "    if len(labels) < expected:\n",
    "        labels.extend([\"Other\"] * (expected - len(labels)))\n",
    "\n",
    "    # 초과하면 앞 expected개만 사용\n",
    "    return labels[:expected]\n",
    "\n",
    "import re\n",
    "SPLIT_PUNCS = re.compile(r\"[.!?;:]+\")\n",
    "\n",
    "def split_sentences(paragraph: str) -> list[str]:\n",
    "    \"\"\"문장 구분 기호로 문단을 분할\"\"\"\n",
    "    sentences = SPLIT_PUNCS.split(paragraph.strip())\n",
    "    return [s.strip() for s in sentences if s.strip()]  # 빈 문자열 제거\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8e7cb",
   "metadata": {},
   "source": [
    " Initialising ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "print(\"▸ Initialising ChromaDB …\")\n",
    "CHROMA_DIR = './chroma_test_db'\n",
    "print(CHROMA_DIR)\n",
    "client = chromadb.PersistentClient(path=CHROMA_DIR)\n",
    "col_sent  = client.get_or_create_collection(name=COLLECTION_SENT_NAME)\n",
    "col_rel   = client.get_or_create_collection(name=COLLECTION_RELA_NAME)\n",
    "col_sent.count(), col_rel.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba10522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, hashlib\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict\n",
    "from typing import Dict, Iterable, Tuple, Optional\n",
    "\n",
    "def generate_sent_nodes(dataset: DatasetDict) -> Iterable[Dict]:\n",
    "    for sample in tqdm(dataset, total=len(dataset), desc=\"Generating nodes\"):\n",
    "        paper_id = sample[\"id\"]\n",
    "    \n",
    "        for sec_idx, (sec_name, sec_content) in enumerate(\n",
    "                zip(sample[\"full_text\"][\"section_name\"],\n",
    "                    sample[\"full_text\"][\"paragraphs\"])):\n",
    "    \n",
    "            for para_idx, para in enumerate(sec_content):\n",
    "                sentences = split_sentences(para.strip())\n",
    "                prev: Optional[Tuple[str, str]] = None\n",
    "\n",
    "                for sent_idx, sent in enumerate(filter(None, sentences)):\n",
    "                    para_id = f\"{paper_id}/sec{sec_idx}/para{para_idx}\"\n",
    "                    path = f\"{para_id}/sent{sent_idx}\"\n",
    "                    sid  = hashlib.sha1(path.encode()).hexdigest()\n",
    "\n",
    "                    yield {\n",
    "                        \"sid\":    sid,\n",
    "                        \"sent\":   sent.strip(),\n",
    "                        \"prev\":   prev,\n",
    "                        \"meta\":   dict(paper_id=paper_id, sec_idx=sec_idx,\n",
    "                                        para_idx=para_idx, sent_idx=sent_idx)\n",
    "                    }\n",
    "                    prev = dict(sid=sid, sent=sent.strip())  # skips first sentence in paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "from typing import List, Tuple\n",
    "\n",
    "def flush_sentence(ids, sents, metas):\n",
    "    if not ids:\n",
    "        return\n",
    "\n",
    "    embs = ENC_MODEL.encode(\n",
    "        sents,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        batch_size=len(sents),\n",
    "        show_progress_bar=False,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    col_sent.upsert(\n",
    "        ids=ids,\n",
    "        embeddings=embs,\n",
    "        documents=sents,\n",
    "        metadatas=metas\n",
    "    )\n",
    "\n",
    "\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# if not logger.handlers:\n",
    "#     handler = logging.StreamHandler(sys.stdout)\n",
    "#     formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "#     handler.setFormatter(formatter)\n",
    "#     logger.addHandler(handler)\n",
    "# async def flush_pairs_async(pairs, batch_size: int = 50) -> None:\n",
    "#     if not pairs:\n",
    "#         logging.debug(\"flush_pairs_async: Received empty pair buffer. Skipping.\")\n",
    "#         return\n",
    "\n",
    "#     ab_pairs = [(pair['prev_sen'], pair['curr_sen']) for pair in pairs]\n",
    "#     logging.info(f\"↪ [flush_pairs_async] Processing {len(pairs)}\")\n",
    "\n",
    "#     # TODO : LLM labeling\n",
    "\n",
    "#     for pair in pairs:\n",
    "#         prev_sid = pair['prev_sid']\n",
    "#         curr_sid = pair['curr_sid']\n",
    "#         rel_id = f\"{prev_sid}|{curr_sid}\"\n",
    "#         try:\n",
    "#             col_rel.upsert(\n",
    "#                 ids        = [rel_id],\n",
    "#                 documents  = [f\"{prev_sid} <REL> {curr_sid}\"],\n",
    "#                 embeddings = [list(np.ndarray(arange(11)))], # [vec],\n",
    "#                 metadatas  = [{\n",
    "#                     # \"relation_label\": lab,\n",
    "#                     \"paper_id\": pair['meta']['paper_id'],\n",
    "#                     \"sec_idx\": pair['meta']['sec_idx'],\n",
    "#                     \"para_idx\": pair['meta']['para_idx'],\n",
    "#                     \"sid_src\": prev_sid,\n",
    "#                     \"sid_dst\": curr_sid,\n",
    "#                 }],\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             logging.warning(f\"[Upsert Failed] ID: {rel_id} — {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def process_dataset(nodes, batch_sent=32, batch_pairs=50):\n",
    "    ids, sents, metas = [], [], []\n",
    "    pair_buf          = []\n",
    "    tasks             = []\n",
    "\n",
    "    for node in nodes:\n",
    "        ids.append(node[\"sid\"])\n",
    "        sents.append(node[\"sent\"])\n",
    "        metas.append(node[\"meta\"])\n",
    "\n",
    "        if node[\"prev\"]:\n",
    "            # print(f\"Processing pair: {node['prev']['sid']} -> {node['sid']}\")\n",
    "            pair_buf.append(dict(meta=node['meta'],\n",
    "                             prev_sid=node[\"prev\"][\"sid\"],  curr_sid=node[\"sid\"],\n",
    "                             prev_sen=node[\"prev\"][\"sent\"], curr_sen=node[\"sent\"]))\n",
    "\n",
    "        if len(ids) >= batch_sent:\n",
    "            flush_sentence(ids, sents, metas)\n",
    "            ids, sents, metas = [], [], []\n",
    "\n",
    "        # if len(pair_buf) >= batch_pairs:\n",
    "        #     # 병렬 실행 예약\n",
    "        #     tasks.append(asyncio.create_task(flush_pairs_async(pair_buf)))\n",
    "        #     pair_buf = []\n",
    "\n",
    "    if ids:\n",
    "        flush_sentence(ids, sents, metas)\n",
    "    # if pair_buf:\n",
    "        # tasks.append(asyncio.create_task(flush_pairs_async(pair_buf)))\n",
    "\n",
    "    # 모든 병렬 작업 종료 대기\n",
    "    if tasks:\n",
    "        await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"▸ Embedding sentences + relations …\")\n",
    "subset = trainset.select(range(2))  # for testing\n",
    "await process_dataset(generate_sent_nodes(subset))\n",
    "print(\"▸ Done embedding sentences + relations.\")\n",
    "print(f\"▸ Stored sentences: {col_sent.count():,}\")\n",
    "# print(f\"▸ Stored vectors   : {col_rel.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tmporary testing\n",
    "def semantic_query(query: str, paper_id: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    paper_id 에 해당하는 문서 내부에서만 최근접 문장 k개 검색\n",
    "    \"\"\"\n",
    "    q_vec = ENC_MODEL.encode(query, normalize_embeddings=True)\n",
    "    res   = col_sent.query(\n",
    "        query_embeddings=[q_vec],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "        where={\"paper_id\": paper_id}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[Doc = {paper_id}]  {query!r}\")\n",
    "    for doc, dist, sid, meta in zip(res[\"documents\"][0], res[\"distances\"][0], res[\"ids\"][0], res[\"metadatas\"][0]):\n",
    "        print(f\" • doc = {doc}\")# dist={dist:.4f}  sid={sid[:8]} text≈{meta.get('sent_idx')}  rel={meta.get('relation_label')}\")\n",
    "\n",
    "\n",
    "# sample = trainset[4]\n",
    "# print(len(sample['qas']['question']))\n",
    "# query = sample['qas']['question'][0]\n",
    "# id = sample['id']\n",
    "# topk= 5\n",
    "# semantic_query(query, id, k=topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd766f4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the collections \n",
    "from chromadb import PersistentClient\n",
    "print(CHROMA_DIR)\n",
    "client = PersistentClient(path='./demo_chroma_db')\n",
    "col_sent =client.get_or_create_collection(name=COLLECTION_SENT_NAME)\n",
    "col_rel = client.get_or_create_collection(name=COLLECTION_RELA_NAME)\n",
    "col_rel.count(), col_sent.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347b2bb",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4819cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1) ChromaDB 에서 문장·관계 로드\n",
    "\n",
    "# 1.1) 문장 노드 피처 및 메타\n",
    "sent_data   = col_sent.get(include=[\"embeddings\",\"metadatas\"])\n",
    "sent_metas  = sent_data[\"metadatas\"]                                    # 각 dict에 \"paper_id\",\"sid\",\"para_id\",\"sent_idx\" 포함\n",
    "node_ids    = sent_data[\"ids\"]                                          # List[str]\n",
    "feats       = torch.tensor(sent_data[\"embeddings\"], dtype=torch.float)  # [N,768]\n",
    "\n",
    "tmp_tree = defaultdict( # paper\n",
    "    lambda: defaultdict( # section\n",
    "        lambda: defaultdict(list) # paragraph[sentences]\n",
    "    )\n",
    ")\n",
    "\n",
    "for idx, meta in enumerate(sent_metas):\n",
    "    paper_idx = meta.get(\"paper_id\", None)  # 논문 ID\n",
    "    sec_idx = meta.get(\"sec_idx\", None)  # not section title... in qasper\n",
    "    para_idx = meta.get(\"para_idx\", None)\n",
    "    # sent_idx = meta.get(\"sent_idx\", None)  # 문장 인덱스\n",
    "\n",
    "\n",
    "    sid = node_ids[idx]\n",
    "\n",
    "    tmp_tree[paper_idx][sec_idx][para_idx].append(sid)\n",
    "\n",
    "# 기존 paragraph 연결 외에, section·paper 단계까지 확장\n",
    "HIER_REL = [\"paragraph\", \"section\", \"paper\"]\n",
    "hier2idx = {name: i for i, name in enumerate(HIER_REL)}\n",
    "\n",
    "# sid → global index\n",
    "id2idx = {sid:i for i,sid in enumerate(node_ids)}\n",
    "data_list = []  # 최종 서브그래프 모음\n",
    "\n",
    "for pid, secs in tmp_tree.items():\n",
    "    nodes, edges, types = set(), [], []\n",
    "\n",
    "    # 2.1) paragraph-level 엣지\n",
    "    for sec_idx, paras in secs.items():\n",
    "        for para_idx, sid_list in paras.items():\n",
    "            # 각 문단 내 문장 인덱스(글로벌) 변환 & 정렬\n",
    "            idxs = [ id2idx[sid] for sid in sid_list ]\n",
    "            idxs.sort(key=lambda i: sent_metas[i][\"sent_idx\"])\n",
    "            # 인접 문장쌍 연결 (양방향)\n",
    "            for u, v in zip(idxs, idxs[1:]):\n",
    "                nodes.update((u, v))\n",
    "                edges.append((u, v)); types.append(hier2idx[\"paragraph\"])\n",
    "                edges.append((v, u)); types.append(hier2idx[\"paragraph\"])\n",
    "\n",
    "    # 2.2) section-level 엣지\n",
    "    for sec_idx, paras in secs.items():\n",
    "        # 각 문단의 첫 문장만 추출해 adjacent 연결\n",
    "        heads = []\n",
    "        for para_idx in sorted(paras.keys()):\n",
    "            if paras[para_idx]:\n",
    "                heads.append( paras[para_idx][0] )\n",
    "        idxs = [ id2idx[sid] for sid in heads ]\n",
    "        for u, v in zip(idxs, idxs[1:]):\n",
    "            nodes.update((u, v))\n",
    "            edges.append((u, v)); types.append(hier2idx[\"section\"])\n",
    "            edges.append((v, u)); types.append(hier2idx[\"section\"])\n",
    "\n",
    "    # 2.3) paper-level 엣지\n",
    "    # 각 섹션의 첫 문단 첫 문장을 연결\n",
    "    section_heads = []\n",
    "    for sec_idx in sorted(secs.keys()):\n",
    "        paras = secs[sec_idx]\n",
    "        if not paras: continue\n",
    "        first_para = sorted(paras.keys())[0]\n",
    "        if paras[first_para]:\n",
    "            section_heads.append( paras[first_para][0] )\n",
    "    idxs = [ id2idx[sid] for sid in section_heads ]\n",
    "    for u, v in zip(idxs, idxs[1:]):\n",
    "        nodes.update((u, v))\n",
    "        edges.append((u, v)); types.append(hier2idx[\"paper\"])\n",
    "        edges.append((v, u)); types.append(hier2idx[\"paper\"])\n",
    "\n",
    "    # 2.4) Data 객체 생성\n",
    "    if not edges:\n",
    "        continue\n",
    "    uniq_nodes = sorted(nodes)\n",
    "    g2l = {g: i for i, g in enumerate(uniq_nodes)}\n",
    "    x_sub = feats[uniq_nodes]                              # [n_sub, 768]\n",
    "    edge_index = torch.tensor([\n",
    "        [ g2l[u] for u, _ in edges ],\n",
    "        [ g2l[v] for _, v in edges ]\n",
    "    ], dtype=torch.long)\n",
    "    edge_type = torch.tensor(types, dtype=torch.long)      # [E]\n",
    "\n",
    "    data_list.append(Data(x=x_sub,\n",
    "                         edge_index=edge_index,\n",
    "                         edge_type=edge_type))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# 1) GraphSAGE 정의 (relation weight 제거)\n",
    "class GraphSAGE(MessagePassing):\n",
    "    def __init__(self, in_dim, hid_dim):\n",
    "        super().__init__(aggr=\"mean\")\n",
    "        self.lin_self  = torch.nn.Linear(in_dim, hid_dim)\n",
    "        self.lin_neigh = torch.nn.Linear(in_dim, hid_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 자기 표현\n",
    "        h_self  = self.lin_self(x)\n",
    "        # 이웃 메시지 집계\n",
    "        h_neigh = self.propagate(edge_index, x=x)\n",
    "        return F.relu(h_self + h_neigh)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        # 단순히 선형 변환된 이웃 임베딩 반환\n",
    "        return self.lin_neigh(x_j)\n",
    "\n",
    "\n",
    "# 2) 학습 세팅\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model     = GraphSAGE(in_dim=768, hid_dim=256).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs    = 1000\n",
    "loader    = DataLoader(data_list, batch_size=1, shuffle=True)\n",
    "\n",
    "# 3) Contrastive-style 학습 루프\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3.1) 노드 임베딩 계산\n",
    "        h = model(batch.x, batch.edge_index)            # [n_sub, 256]\n",
    "\n",
    "        # 3.2) Positive edge similarity\n",
    "        src, dst = batch.edge_index                     # [2, E]\n",
    "        pos_sim  = (h[src] * h[dst]).sum(dim=1)          # [E]\n",
    "\n",
    "        # 3.3) Negative sampling\n",
    "        neg_idx  = negative_sampling(\n",
    "            edge_index     = batch.edge_index,\n",
    "            num_nodes      = h.size(0),\n",
    "            num_neg_samples= src.size(0)\n",
    "        )\n",
    "        ns, nd   = neg_idx\n",
    "        neg_sim  = (h[ns] * h[nd]).sum(dim=1)            # [E]\n",
    "\n",
    "        # 3.4) Loss 계산\n",
    "        loss_pos = - F.logsigmoid(pos_sim).mean()\n",
    "        loss_neg = - F.logsigmoid(-neg_sim).mean()\n",
    "        loss     = loss_pos + loss_neg\n",
    "\n",
    "        # 3.5) 역전파 및 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"[Epoch {ep:02d}] Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for pid, secs in tmp_tree.items():\n",
    "    for sec_idx, paras in secs.items():\n",
    "        for para_idx, sids in paras.items():\n",
    "            rows.append({\n",
    "                \"paper_id\": pid,\n",
    "                \"section\": sec_idx,\n",
    "                \"paragraph\": para_idx,\n",
    "                \"num_sentences\": len(sids)\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6afbb7",
   "metadata": {},
   "source": [
    "## 4. LangGraph State and Node config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326760d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sent = client.get_or_create_collection(COLLECTION_SENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% LangGraph & 노드 정의 (이하는 기존과 동일)\n",
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "from typing import TypedDict\n",
    "\n",
    "# 모델 설정\n",
    "encoder     = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "ce_reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "llm         = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=os.getenv('GOOGLE_API_KEY'),\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# State Schema\n",
    "class InputState(TypedDict):\n",
    "    paper_id: str\n",
    "    question: str\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    paper_id: str\n",
    "    answer: str\n",
    "    top_docs: list[str]\n",
    "    top_metadatas: list[dict]\n",
    "\n",
    "class OverallState(InputState, OutputState):\n",
    "    retrieved_docs: list[str]\n",
    "    retrieved_metadatas: list[dict]\n",
    "\n",
    "# retrieve 노드: 문장 컬렉션에서 검색\n",
    "def retrieve(state: OverallState) -> OverallState:\n",
    "    assert col_sent is not None, \"Sentence-level collection not found\"\n",
    "    q        = state[\"question\"]\n",
    "    paper_id = state[\"paper_id\"]\n",
    "    # print(f\"Retrieving sentences for paper {paper_id!r} with query {q!r}\")\n",
    "    # 1) 질의 임베딩\n",
    "    q_emb = encoder.encode(q)\n",
    "    # 2) col_sent에서 논문별 필터링 후 top-20 문장 검색\n",
    "    res = col_sent.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=10,\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "        where={\"paper_id\": paper_id}\n",
    "    )\n",
    "    # print(f\"Retrived sents : {res['documents'][0]}\")\n",
    "    # print(f\"Retrieved {len(res['documents'][0])} sentences for paper {paper_id!r} with query {q!r}\")\n",
    "    # Chromadb 문법상 [0]으로 추출\n",
    "    state[\"retrieved_docs\"]      = res[\"documents\"][0]\n",
    "    state[\"retrieved_metadatas\"] = res[\"metadatas\"][0]\n",
    "    return state\n",
    "\n",
    "# rerank / generate 노드는 기존과 동일\n",
    "def rerank(state: OverallState) -> OverallState:\n",
    "    docs      = state[\"retrieved_docs\"]\n",
    "    metadatas = state[\"retrieved_metadatas\"]\n",
    "    q         = state[\"question\"]\n",
    "\n",
    "    scores = ce_reranker.predict([(q, d) for d in docs])\n",
    "    ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    state[\"top_docs\"]      = [docs[i] for i, _ in ranked[:6]]\n",
    "    state[\"top_metadatas\"] = [metadatas[i] for i, _ in ranked[:6]]\n",
    "    return state\n",
    "\n",
    "def generate(state: OverallState) -> OverallState:\n",
    "    ctx = \"\\n\\n\".join(state[\"top_docs\"])\n",
    "    prompt = (\n",
    "        f\"Prompt: Answer based on context below. If you don't know, say 'unanswerable'.\\n\"\n",
    "        f\"Context:\\n{ctx}\\n\\nQuestion: {state['question']}\\nAnswer:\"\n",
    "    )\n",
    "    resp = llm.invoke(prompt)\n",
    "    state[\"answer\"] = resp.content\n",
    "    return state\n",
    "\n",
    "# 파이프라인 빌드\n",
    "builder = StateGraph(state_schema=OverallState, input=InputState, output=OutputState)\n",
    "builder.add_node(\"retrieve\", retrieve)\n",
    "builder.add_node(\"rerank\",   rerank)\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.set_entry_point(\"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"rerank\")\n",
    "builder.add_edge(\"rerank\",   \"generate\")\n",
    "builder.set_finish_point(\"generate\")\n",
    "\n",
    "rag_pipeline = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 1) 데이터 로드 (검증용 subset)\n",
    "dataset = load_dataset('allenai/qasper', split='train', cache_dir='./data/Qasper/qasper_cache')\n",
    "# subset  = dataset.shuffle(seed=42).select(range(int(len(dataset)*0.1)))  # 10%\n",
    "\n",
    "# 2) helper functions\n",
    "def normalize(text: str) -> str:\n",
    "    return ''.join(c.lower() for c in text if c.isalnum() or c.isspace()).strip()\n",
    "\n",
    "def find_gold_para_ids(sample, gold_evids):\n",
    "    paper_id = sample[\"id\"]\n",
    "    gold_ids = set()\n",
    "    # full_text['paragraphs'] 는 섹션별로 [문단1, 문단2, …] 리스트\n",
    "    sections = sample[\"full_text\"][\"paragraphs\"]\n",
    "    for ev in gold_evids:\n",
    "        ev_norm = ev.strip()\n",
    "        for sec_idx, paras in enumerate(sections):\n",
    "            for para_idx, para_text in enumerate(paras):\n",
    "                if ev_norm in para_text:\n",
    "                    para_id = f\"sec{sec_idx}/para{para_idx}\"\n",
    "                    gold_ids.add(f\"{paper_id}_{para_id}\")\n",
    "                    # 하나의 evidence는 하나의 문단만 매핑하므로 찾으면 바로 빠져나감\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "    return gold_ids\n",
    "\n",
    "def compute_retrieval_metrics(pred: list[str], gold: set[str]):\n",
    "    pred_set = set(pred)\n",
    "    tp = len(pred_set & gold)\n",
    "    prec = tp / len(pred_set) if pred_set else 0.0\n",
    "    rec  = tp / len(gold)     if gold     else 0.0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "def compute_answer_metrics(pred_ans: str, gold_texts: list[str], gold_yesno: list[bool], gold_unans: list[bool]):\n",
    "    norm_pred = normalize(pred_ans)\n",
    "    # Exact Match over all annotators\n",
    "    em = any(norm_pred == normalize(gt) for gt in gold_texts)\n",
    "    # yes/no\n",
    "    if any(gold_yesno):\n",
    "        yn_pred = norm_pred.startswith('yes') or norm_pred.startswith('no')\n",
    "        yn_acc  = int(yn_pred and norm_pred.split()[0] == ('yes' if any(gold_yesno) else 'no'))\n",
    "    else:\n",
    "        yn_acc = None\n",
    "    # unanswerable\n",
    "    if any(gold_unans):\n",
    "        un_pred = norm_pred.startswith('unanswerable')\n",
    "        un_acc  = int(un_pred)\n",
    "    else:\n",
    "        un_acc = None\n",
    "    return int(em), yn_acc, un_acc\n",
    "\n",
    "# 3) evaluation loop\n",
    "records = []\n",
    "for sample in tqdm(subset, desc=\"Evaluating\"):\n",
    "    pid       = sample[\"id\"]\n",
    "    questions = sample[\"qas\"][\"question\"]\n",
    "    answers   = sample[\"qas\"][\"answers\"]\n",
    "    \n",
    "    for q_text, ans_block in zip(questions, answers):\n",
    "        # 3.1) RAG-invoke\n",
    "        out = rag_pipeline.invoke({\"paper_id\": pid, \"question\": q_text})\n",
    "        pred_ans        = out[\"answer\"].strip()\n",
    "        pred_metadatas  = out[\"top_metadatas\"]\n",
    "        # predicted evidence IDs\n",
    "        pred_ids = []\n",
    "        for md in pred_metadatas:\n",
    "            pid     = md[\"paper_id\"]\n",
    "            sec_idx = md[\"sec_idx\"]\n",
    "            para_idx= md[\"para_idx\"]\n",
    "            para_id = f\"sec{sec_idx}/para{para_idx}\"\n",
    "            pred_ids.append(f\"{pid}_{para_id}\")\n",
    "        # in format of \"paper_id_para_idx\"\n",
    "        \n",
    "        # 3.2) gold aggregation\n",
    "        gold_texts, gold_evids, gold_yesno, gold_unans = [], [], [], []\n",
    "        # del gold_texts, gold_yesno, gold_unans  # clear previous\n",
    "        for ann in ans_block[\"answer\"]:\n",
    "            if ann.get(\"answer\"):\n",
    "                gold_texts.append(ann[\"answer\"])\n",
    "            if ann.get(\"evidence\"):\n",
    "                gold_evids.extend(ann[\"evidence\"])\n",
    "            if ann.get(\"yes_no\") is not None:\n",
    "                gold_yesno.append(ann[\"yes_no\"])\n",
    "            if ann.get(\"unanswerable\") is not None:\n",
    "                gold_unans.append(ann[\"unanswerable\"])\n",
    "        \n",
    "        gold_ids = find_gold_para_ids(sample, gold_evids)\n",
    "        \n",
    "        # 3.3) metrics\n",
    "        prec, rec, f1 = compute_retrieval_metrics(pred_ids, gold_ids)\n",
    "        em, yn_acc, un_acc = compute_answer_metrics(pred_ans, gold_texts, gold_yesno, gold_unans)\n",
    "        \n",
    "        records.append({\n",
    "            \"paper_id\":           pid,\n",
    "            \"question\":           q_text,\n",
    "            \"precision\":          prec,\n",
    "            \"recall\":             rec,\n",
    "            \"f1\":                 f1,\n",
    "            \"exact_match\":        em,\n",
    "            \"yes_no_acc\":         yn_acc,\n",
    "            \"unans_acc\":          un_acc\n",
    "        })\n",
    "\n",
    "# 4) DataFrame 생성 및 paper별 집계\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# 질문 단위 전체 평균\n",
    "overall = df.mean(numeric_only=True).round(4)\n",
    "print(\"=== Overall ===\")\n",
    "print(overall.to_dict())\n",
    "\n",
    "# paper_id 별 평균\n",
    "by_paper = (\n",
    "    df\n",
    "    .groupby(\"paper_id\")\n",
    "    .agg({\n",
    "        \"precision\":   \"mean\",\n",
    "        \"recall\":      \"mean\",\n",
    "        \"f1\":          \"mean\",\n",
    "        \"exact_match\": \"mean\",\n",
    "        \"yes_no_acc\":  \"mean\",\n",
    "        \"unans_acc\":   \"mean\"\n",
    "    })\n",
    "    .round(4)\n",
    "    .reset_index()\n",
    ")\n",
    "print(\"\\n=== By Paper ===\")\n",
    "print(by_paper)\n",
    "\n",
    "# 5) 결과 저장\n",
    "df.to_csv(\"qasper_rag_eval_per_question.csv\", index=False)\n",
    "by_paper.to_csv(\"qasper_rag_eval_by_paper.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd581ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 1) 데이터 로드 (검증용 subset)\n",
    "dataset = load_dataset('allenai/qasper', split='train', cache_dir='./data/Qasper/qasper_cache')\n",
    "# subset  = dataset.shuffle(seed=42).select(range(int(len(dataset)*0.1)))  # 10%\n",
    "\n",
    "# 2) helper functions\n",
    "def normalize(text: str) -> str:\n",
    "    return ''.join(c.lower() for c in text if c.isalnum() or c.isspace()).strip()\n",
    "\n",
    "def find_gold_para_ids(sample, gold_evids):\n",
    "    paper_id = sample[\"id\"]\n",
    "    gold_ids = set()\n",
    "    # full_text['paragraphs'] 는 섹션별로 [문단1, 문단2, …] 리스트\n",
    "    sections = sample[\"full_text\"][\"paragraphs\"]\n",
    "    for ev in gold_evids:\n",
    "        ev_norm = ev.strip()\n",
    "        for sec_idx, paras in enumerate(sections):\n",
    "            for para_idx, para_text in enumerate(paras):\n",
    "                if ev_norm in para_text:\n",
    "                    para_id = f\"sec{sec_idx}/para{para_idx}\"\n",
    "                    gold_ids.add(f\"{paper_id}_{para_id}\")\n",
    "                    # 하나의 evidence는 하나의 문단만 매핑하므로 찾으면 바로 빠져나감\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "    return gold_ids\n",
    "\n",
    "def compute_retrieval_metrics(pred: list[str], gold: set[str]):\n",
    "    pred_set = set(pred)\n",
    "    tp = len(pred_set & gold)\n",
    "    prec = tp / len(pred_set) if pred_set else 0.0\n",
    "    rec  = tp / len(gold)     if gold     else 0.0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "def compute_answer_metrics(pred_ans: str, gold_texts: list[str], gold_yesno: list[bool], gold_unans: list[bool]):\n",
    "    norm_pred = normalize(pred_ans)\n",
    "    # Exact Match over all annotators\n",
    "    em = any(norm_pred == normalize(gt) for gt in gold_texts)\n",
    "    # yes/no\n",
    "    if any(gold_yesno):\n",
    "        yn_pred = norm_pred.startswith('yes') or norm_pred.startswith('no')\n",
    "        yn_acc  = int(yn_pred and norm_pred.split()[0] == ('yes' if any(gold_yesno) else 'no'))\n",
    "    else:\n",
    "        yn_acc = None\n",
    "    # unanswerable\n",
    "    if any(gold_unans):\n",
    "        un_pred = norm_pred.startswith('unanswerable')\n",
    "        un_acc  = int(un_pred)\n",
    "    else:\n",
    "        un_acc = None\n",
    "    return int(em), yn_acc, un_acc\n",
    "\n",
    "# 3) evaluation loop\n",
    "records = []\n",
    "for sample in tqdm(subset, desc=\"Evaluating\"):\n",
    "    pid       = sample[\"id\"]\n",
    "    questions = sample[\"qas\"][\"question\"]\n",
    "    answers   = sample[\"qas\"][\"answers\"]\n",
    "    \n",
    "    for q_text, ans_block in zip(questions, answers):\n",
    "        # 3.1) RAG-invoke\n",
    "        out = rag_pipeline.invoke({\"paper_id\": pid, \"question\": q_text})\n",
    "        pred_ans        = out[\"answer\"].strip()\n",
    "        pred_metadatas  = out[\"top_metadatas\"]\n",
    "        # predicted evidence IDs\n",
    "        pred_ids = []\n",
    "        for md in pred_metadatas:\n",
    "            pid     = md[\"paper_id\"]\n",
    "            sec_idx = md[\"sec_idx\"]\n",
    "            para_idx= md[\"para_idx\"]\n",
    "            para_id = f\"sec{sec_idx}/para{para_idx}\"\n",
    "            pred_ids.append(f\"{pid}_{para_id}\")\n",
    "        # in format of \"paper_id_para_idx\"\n",
    "        \n",
    "        # 3.2) gold aggregation\n",
    "        gold_texts, gold_evids, gold_yesno, gold_unans = [], [], [], []\n",
    "        # del gold_texts, gold_yesno, gold_unans  # clear previous\n",
    "        for ann in ans_block[\"answer\"]:\n",
    "            if ann.get(\"answer\"):\n",
    "                gold_texts.append(ann[\"answer\"])\n",
    "            if ann.get(\"evidence\"):\n",
    "                gold_evids.extend(ann[\"evidence\"])\n",
    "            if ann.get(\"yes_no\") is not None:\n",
    "                gold_yesno.append(ann[\"yes_no\"])\n",
    "            if ann.get(\"unanswerable\") is not None:\n",
    "                gold_unans.append(ann[\"unanswerable\"])\n",
    "        \n",
    "        gold_ids = find_gold_para_ids(sample, gold_evids)\n",
    "        \n",
    "        # 3.3) metrics\n",
    "        prec, rec, f1 = compute_retrieval_metrics(pred_ids, gold_ids)\n",
    "        em, yn_acc, un_acc = compute_answer_metrics(pred_ans, gold_texts, gold_yesno, gold_unans)\n",
    "        \n",
    "        records.append({\n",
    "            \"paper_id\":           pid,\n",
    "            \"question\":           q_text,\n",
    "            \"precision\":          prec,\n",
    "            \"recall\":             rec,\n",
    "            \"f1\":                 f1,\n",
    "            \"exact_match\":        em,\n",
    "            \"yes_no_acc\":         yn_acc,\n",
    "            \"unans_acc\":          un_acc\n",
    "        })\n",
    "\n",
    "# 4) DataFrame 생성 및 paper별 집계\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# 질문 단위 전체 평균\n",
    "overall = df.mean(numeric_only=True).round(4)\n",
    "print(\"=== Overall ===\")\n",
    "print(overall.to_dict())\n",
    "\n",
    "# paper_id 별 평균\n",
    "by_paper = (\n",
    "    df\n",
    "    .groupby(\"paper_id\")\n",
    "    .agg({\n",
    "        \"precision\":   \"mean\",\n",
    "        \"recall\":      \"mean\",\n",
    "        \"f1\":          \"mean\",\n",
    "        \"exact_match\": \"mean\",\n",
    "        \"yes_no_acc\":  \"mean\",\n",
    "        \"unans_acc\":   \"mean\"\n",
    "    })\n",
    "    .round(4)\n",
    "    .reset_index()\n",
    ")\n",
    "print(\"\\n=== By Paper ===\")\n",
    "print(by_paper)\n",
    "\n",
    "# 5) 결과 저장\n",
    "df.to_csv(\"qasper_rag_eval_per_question.csv\", index=False)\n",
    "by_paper.to_csv(\"qasper_rag_eval_by_paper.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
